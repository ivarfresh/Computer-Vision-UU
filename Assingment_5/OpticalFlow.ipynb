{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa8b15b-f427-427b-ade8-0dbaf5466c16",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6884604a-a686-4e5b-8013-92b07f92becf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 13:45:28.025705: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-16 13:45:28.839944: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/riemer/enter/lib/python3.9/site-packages/cv2/../../lib64::/usr/local/lib:/usr/local/lib\n",
      "2023-04-16 13:45:28.839972: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-16 13:45:30.560684: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/riemer/enter/lib/python3.9/site-packages/cv2/../../lib64::/usr/local/lib:/usr/local/lib\n",
      "2023-04-16 13:45:30.561728: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/riemer/enter/lib/python3.9/site-packages/cv2/../../lib64::/usr/local/lib:/usr/local/lib\n",
      "2023-04-16 13:45:30.561764: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabebe1f-b7ea-4fca-b4c6-5ccdc926300b",
   "metadata": {},
   "source": [
    "## Optical Flow Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1924420a-170b-44e7-83d4-295b249aaabc",
   "metadata": {},
   "source": [
    "### Middle frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b17e2-e1d9-45c1-9e07-a4f67a8a8ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = 'HMDB51/video_data'\n",
    "\n",
    "# # Set the parameters for the optical flow calculation\n",
    "# lk_params = dict(winSize=(15, 15),\n",
    "#                  maxLevel=2,\n",
    "#                  criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# for path, dirs, files in os.walk(root):\n",
    "#     for file in files:\n",
    "#         video_path = os.path.join(path, file)\n",
    "#         cap = cv2.VideoCapture(video_path)\n",
    "#         length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "#         # obtain the middle frame\n",
    "#         frame_num = length//2\n",
    "#         cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "#         _, frame = cap.read()\n",
    "        \n",
    "#         cv2.imwrite('middle.png', frame)\n",
    "        \n",
    "#         prev_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "#         cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num+1)\n",
    "#         _, frame = cap.read()\n",
    "        \n",
    "#         cv2.imwrite('next.png', frame)\n",
    "        \n",
    "#         cap.release()\n",
    "        \n",
    "#         gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "              \n",
    "#         # Calculate the optical flow between the current and previous frames\n",
    "#         flow = cv2.calcOpticalFlowFarneback(prev_frame, gray_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "#         # Compute the magnitude and direction of the optical flow\n",
    "#         magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "#         # Normalize the magnitude to a range between 0 and 255\n",
    "#         magnitude_norm = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "#         # Convert the magnitude to a grayscale image\n",
    "#         magnitude_image = np.uint8(magnitude_norm)\n",
    "\n",
    "#         # Save the optical flow image\n",
    "#         new_path = path.replace('video_data', 'optical_flow')\n",
    "#         os.makedirs(new_path, exist_ok=True)\n",
    "#         optical_flow_path = os.path.join(new_path, f'{file}_{i}.png')\n",
    "#         cv2.imwrite(optical_flow_path, magnitude_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45b5d96f-a15e-416b-a9de-2102e7a0f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'HMDB51/video_data'\n",
    "\n",
    "for path, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        video_path = os.path.join(path, file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # obtain the middle frame number\n",
    "        frame_num = length//2\n",
    "        \n",
    "        # Read the middle frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        _, middle_frame = cap.read()\n",
    "        prev_frame = cv2.cvtColor(middle_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Read the subsequent frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num+1)\n",
    "        _, next_frame = cap.read()        \n",
    "        next_frame = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        cap.release()\n",
    "        \n",
    "        # Create a mask with max saturation\n",
    "        mask = np.zeros_like(middle_frame)\n",
    "        mask[..., 1] = 255\n",
    "                      \n",
    "        # Calculate the optical flow between the middle frame and the next frame\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_frame, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        # Compute the magnitude and direction of the optical flow\n",
    "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        \n",
    "        # Set image hue according to the optical flow direction\n",
    "        mask[..., 0] = angle * 180 / np.pi / 2\n",
    "        \n",
    "        # Set image value according to the optical flow magnitude\n",
    "        mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        \n",
    "        # Convert to RGB\n",
    "        rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        # Save the optical flow image\n",
    "        new_path = path.replace('video_data', 'optical_flow')\n",
    "        os.makedirs(new_path, exist_ok=True)\n",
    "        optical_flow_path = os.path.join(new_path, f'{file[:-4]}.png')\n",
    "        cv2.imwrite(optical_flow_path, rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45dabaa-235a-4666-8a38-f47d5f6a3557",
   "metadata": {},
   "source": [
    "### All frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d86347-a709-4998-99d3-9552bcdeb4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'HMDB51/video_data'\n",
    "\n",
    "# Set the parameters for the optical flow calculation\n",
    "lk_params = dict(winSize=(15, 15),\n",
    "                 maxLevel=2,\n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "for path, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        video_path = os.path.join(path, file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(length)\n",
    "        prev_frame = None\n",
    "\n",
    "        # Loop through each frame in the video and calculate the optical flow\n",
    "        for i in range(length):\n",
    "            # Read the next frame\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if ret:\n",
    "                # Convert the frame to grayscale\n",
    "                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                if prev_frame is not None:\n",
    "                    # Calculate the optical flow between the current and previous frames\n",
    "                    flow = cv2.calcOpticalFlowFarneback(prev_frame, gray_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "                    # Compute the magnitude and direction of the optical flow\n",
    "                    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "                    # Normalize the magnitude to a range between 0 and 255\n",
    "                    magnitude_norm = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "                    # Convert the magnitude to a grayscale image\n",
    "                    magnitude_image = np.uint8(magnitude_norm)\n",
    "\n",
    "                    # Save the optical flow image\n",
    "                    new_path = path.replace('video_data', 'optical_flow')\n",
    "                    os.makedirs(new_path, exist_ok=True)\n",
    "                    optical_flow_path = os.path.join(new_path, f'{file}_{i}.png')\n",
    "                    cv2.imwrite(optical_flow_path, magnitude_image)\n",
    "\n",
    "                # Set the current frame as the previous frame for the next iteration\n",
    "                prev_frame = gray_frame\n",
    "                \n",
    "            else:\n",
    "                cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4538f2-42da-4214-9c7e-9bc716b69d2c",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa9333a0-a746-4f00-a63a-2882d0585090",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_hmdb51 = [\"clap\", \"climb\", \"drink\", \"jump\", \"pour\", \"ride_bike\", \"ride_horse\", \n",
    "        \"run\", \"shoot_bow\", \"smoke\", \"throw\", \"wave\"]\n",
    "\n",
    "TRAIN_TAG, TEST_TAG = 1, 2\n",
    "train_files, test_files = [], []\n",
    "train_labels, test_labels = [], []\n",
    "split_pattern_name = f\"*test_split1.txt\"\n",
    "split_pattern_path = os.path.join('HMDB51/test_train_splits', split_pattern_name)\n",
    "annotation_paths = glob.glob(split_pattern_path)\n",
    "for filepath in annotation_paths:\n",
    "    class_name = '_'.join(filepath.split('/')[-1].split('_')[:-2])\n",
    "    if class_name not in keep_hmdb51:\n",
    "        continue  # skipping the classes that we won't use.\n",
    "    with open(filepath) as fid:\n",
    "        lines = fid.readlines()\n",
    "    for line in lines:\n",
    "        video_filename, tag_string = line.split()\n",
    "        tag = int(tag_string)\n",
    "        if tag == TRAIN_TAG:\n",
    "            train_files.append(video_filename)\n",
    "            train_labels.append(class_name)\n",
    "        elif tag == TEST_TAG:\n",
    "            test_files.append(video_filename)\n",
    "            test_labels.append(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65810e58-66b8-4f70-9c13-609e061d5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "\n",
    "for file in train_files:\n",
    "    vid_name = file[:-4]\n",
    "    for path, _, files in os.walk('HMDB51/optical_flow'):\n",
    "        for file_name in files:\n",
    "            if file_name[:-4] == vid_name:\n",
    "                img = cv2.imread(os.path.join(path, file_name))\n",
    "                img = cv2.resize(img, (112, 112))\n",
    "                x_train.append(img)\n",
    "                \n",
    "for file in test_files:\n",
    "    vid_name = file[:-4]\n",
    "    for path, _, files in os.walk('HMDB51/optical_flow'):\n",
    "        for file_name in files:\n",
    "            if file_name[:-4] == vid_name:\n",
    "                img = cv2.imread(os.path.join(path, file_name))\n",
    "                img = cv2.resize(img, (112, 112))\n",
    "                x_test.append(img)\n",
    "                \n",
    "x_train = np.asarray(x_train)/255.\n",
    "x_test = np.asarray(x_test)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "410f38dd-e8ad-486d-b45b-7401e90be849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict to convert string labels to ints\n",
    "str_to_int = {\"clap\":0, \"climb\":1, \"drink\":2, \"jump\":3, \"pour\":4, \"ride_bike\":5, \"ride_horse\":6, \n",
    "              \"run\":7, \"shoot_bow\":8, \"smoke\":9, \"throw\":10, \"wave\":11}\n",
    "\n",
    "y_train = np.array([str_to_int[label] for label in train_labels])\n",
    "y_test = np.array([str_to_int[label] for label in test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24bc551-1b8c-4df3-b352-24e12fde2cba",
   "metadata": {},
   "source": [
    "## Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a41b328-d5ce-4297-a9b1-f0709a220fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(112, 112, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=12, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715c9a4-b18d-4970-9d0d-d7d9ab9dd4a2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59fa37-0482-43c5-858e-531e875fe09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0836c01a-945e-400f-948e-0a78b30af6d9",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eed6eb-b662-454a-aa62-13562911259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b37f5b-f216-4b1e-bc29-4fee252b1208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
